{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report on Project 2: Continuous Control\n",
    "This file reports the method adopted to train an agent for solving the task of continuous control of a double jointed arm in a Unity environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "- state space: 33 (Continuous)\n",
    "- action space: 4 (Continuous)\n",
    "- reward structure: +0.1 for reaching the goal position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "- The inputs are given to an agent which follows a policy for executing the task. \n",
    "- As the state space has real numbers, implementing a Q table is not possible. Hence, neural networks are used for function approximation.\n",
    "- Also, as the action space is continuous, methods such as DQN cannot be directly implemented.\n",
    "- An initial attempt with the REINFORCE method was made but it was found out that the training is unstable and would be much better methods for getting the desired results. \n",
    "- Therefore, the Deep Deterministic Policy Gradient (DDPG) algorithm which is an Actor-Critic method was used.\n",
    "    - In this method the actor is the agent i.e. policy that takes the state and outputs actions.\n",
    "    - While the Critic evaluates the expected values from a state,action pair i.e. Q value estimator\n",
    "- The structure of the Actor is as follows:\n",
    "    - F1 = ReLU (input_state (states = 33) x 128 neurons)\n",
    "    - F2 = ReLU (F1 x 128 neurons)\n",
    "    - F3 = ReLU (F2 x output_state (actions = 4))\n",
    "- The structure of the Critic is as follows:\n",
    "    - F1 = ReLU (input_state (states = 33) x 128 neurons)\n",
    "    - F2 = ReLU (F1+action_size (=4) x 128 neurons)\n",
    "    - F3 = ReLU (F2 x 1) \n",
    "- Two NNs for actor and critic of same architecture are used: local network (θ_local) and target network (θ_target).\n",
    "- The target network is soft updated using the local network θ_target = τ*θ_local + (1 - τ)*θ_target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- BUFFER_SIZE = 1e5       # replay buffer size\n",
    "- BATCH_SIZE = 128         # minibatch size\n",
    "- GAMMA = 0.99            # discount factor\n",
    "- TAU = 1e-3              # for soft update of target parameters\n",
    "- LR_ACTOR = 2e-4       # Actor Learning Rate\n",
    "- LR_CRITIC = 2e-4       # Critic Learning Rate\n",
    "- maximum number of timesteps per episode =1000\n",
    "- WEIGHT_DECAY = 0 # L2 weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards plot\n",
    "A plot of the average rewards received is seen below:\n",
    "![alt text](images/plot.png \"ABC\")\n",
    "It can be seen that the agent receives higher rewards as the experience i.e. number of episodes increases. \n",
    "\n",
    "Number of episodes needed to solve the environment = 235"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future ideas for improving agents performance\n",
    "- Use a different Neural Network architecture for actor and critic\n",
    "- Implement for distributed learning wiht 20 agents\n",
    "- Implement with other methods such as A3C, PPO, D4PG for faster and improved agent performance.\n",
    "- Implement using Hierarchical reinforcement learning\n",
    "    - Move near the goal roughly\n",
    "    - Reach goal fine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p2_drlnd",
   "language": "python",
   "name": "p2_drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
